{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 History of Deep Learning\n",
    "\n",
    "<center><div> <img src=\"graphics/dp_timeline.png\" alt=\"Drawing\" style=\"width: 1000px;\"/></div> </center>\n",
    "\n",
    "__1943:__ The first mathematical model of a neural network was created by Walter Pitts and Warren McCulloch in 1943 which demonstrated the thought process of the human brain. From here began the journey of deep neural network and deep learning.\n",
    "\n",
    "__1957:__ Frank Rosenblatt submitted a paper titled ‘The Perceptron: A Perceiving and Recognizing Automaton’, which consisted of an algorithm or a method for pattern recognition using a two-layer neural network. His work was mostly based on hardware rather than software. \n",
    "\n",
    "__1965:__ Alexey Ivakhnenko and V.G. Lapa developed the first working neural network. Afterwards, in 1971, Alexey Ivakhnenko created an 8-layer deep neural network which was demonstrated in the computer identification system, Alpha. This was the actual introduction to deep learning using statistical methods. \n",
    "\n",
    "__1979:__ Kunihiko Fukushima developed the ‘Neocognitron’, a multilayered ANN that will recognize visual patterns. His work served as the inspiraton of Convolution Neural Network a few decades later. \n",
    "\n",
    "__1985:__ Terry Sejnowski created NETtalk, a program which learnt how to pronounce English words. Around the same timeframe, David Rumelhart, Geoffrey Hinton, and Ronald J. Williams, released a paper entitle \"Learning Representations by Back-propagating Errors,” which described in greater detail the process of backpropagation. \n",
    "\n",
    "__1998:__ Yann LeCun published a paper entitled “Gradient-Based Learning Applied to Document Recognition”  which in conjunction with the backpropation alogrithm revolutionised an increasinly successfull approach to deep learning. A very popular dataset MNIST (a large database of handwritten digits) was used to show how deep learning models were able to learn abstract information by demonstrating that individual layers in the model identified different features, e.g. one layer finds circles and the next one would find edges etc.  \n",
    "\n",
    "__2009:__ As deep learning models require a tremendous amount of labelled data to train themselves in supervised learning, Fei-Fei Li launched ImageNet, which is a large database of labelled images (14 million) available to researches, educators and students. \n",
    " \n",
    "__2012:__ The results of ‘The Cat Experiment’ conducted by Google Brain were released. This experiment was based on unsupervised learning in which the deep neural network worked with unlabelled data to recognize patterns and features in the images of cats. However, it could only recognize 15% of images correctly.\n",
    "\n",
    "__2014:__ Facebook developed, DeepFace, a deep learning system to identify and tag faces of users in the photographs.\n",
    "\n",
    "The past decade has seen great leaps in the evolution of DL and its ability to learn more abstract information due to a combination of increased data, improved hardware and sofware. Today, Deep Learning is ever-present in our everyday lives (as shown in the previous examples) and as the field continues to evolve, humans become closer in their pursuit of AI. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
